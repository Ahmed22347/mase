{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Introduction\n",
    "\n",
    "In this lab, you will learn how to use the search functionality in the software stack of MASE to implement a Network Architecture Search.\n",
    "\n",
    "There are in total 4 tasks you would need to finish, there is also 1 optional task.\n",
    "\n",
    "## What is Network Architecture Search?\n",
    "\n",
    "The design of a network architecture can greatly impact the performance of the model. Consider the following optimization problem:\n",
    "\n",
    "min_{a∈A} L_{val}(w*(a), a)\n",
    "\n",
    "s.t. w*(a) = argmin_{w}(L_{train}(w, a))\n",
    "\n",
    "\n",
    "For an architecture a sampled from a set of architectures A, we are minimizing the loss value when the architecture a is parameterized by w*(a).\n",
    "\n",
    "Meanwhile, the parameters w*(a) is the particular parameterization that provides the lowest L_{train} given the architecture a.\n",
    "\n",
    "This is the core optimization problem involved in Network Architecture Search, where several approximations can happen. For instance, we can approximate L_{train} or L_{val}, or the min_{a∈A} can be formulated as a reinforcement learning process and so on.\n",
    "## A Handwritten JSC Network\n",
    "\n",
    "We follow a similar procedure of what you have tried in lab3 to setup the dataset, copy and paste the following code snippet to a file, and name it `lab4.py`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# figure out the correct path\n",
    "machop_path = Path(\".\").resolve().parent.parent /\"machop\"\n",
    "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "sys.path.append(str(machop_path))\n",
    "\n",
    "from chop.dataset import MaseDataModule, get_dataset_info\n",
    "from chop.tools.logger import set_logging_verbosity, get_logger\n",
    "\n",
    "from chop.passes.graph.analysis import (\n",
    "    report_node_meta_param_analysis_pass,\n",
    "    profile_statistics_analysis_pass,\n",
    ")\n",
    "from chop.passes.graph import (\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    ")\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.ir.graph.mase_graph import MaseGraph\n",
    "\n",
    "from chop.models import get_model_info, get_model\n",
    "\n",
    "set_logging_verbosity(\"info\")\n",
    "\n",
    "logger = get_logger(\"chop\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "batch_size = 8\n",
    "model_name = \"three_layer_jsc\"\n",
    "dataset_name = \"jsc\"\n",
    "\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "dummy_in = {\"x\": next(iter(data_module.train_dataloader()))[0]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are going to use a slightly different network, so we define it as a Pytorch model, copy and paste this snippet also to `lab4.py`.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> MASE integrates seamlessly with native Pytorch models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from chop.passes.graph.utils import get_parent_name\n",
    "\n",
    "# define a new model\n",
    "class JSC_Three_Linear_Layers(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JSC_Three_Linear_Layers, self).__init__()\n",
    "        self.seq_blocks = nn.Sequential(\n",
    "            nn.BatchNorm1d(16),  # 0\n",
    "            nn.ReLU(16),  # 1\n",
    "            nn.Linear(16, 16),  # linear  2\n",
    "            nn.Linear(16, 16),  # linear  3\n",
    "            nn.Linear(16, 5),   # linear  4\n",
    "            nn.ReLU(5),  # 5\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq_blocks(x)\n",
    "\n",
    "\n",
    "model = JSC_Three_Linear_Layers()\n",
    "\n",
    "# generate the mase graph and initialize node metadata\n",
    "mg = MaseGraph(model=model)\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Modification as a Transformation Pass\n",
    "\n",
    "Similar to what you have done in `lab2`, one can also implement a change in model architecture as a transformation pass:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:output\n",
      "counter:both\n",
      "counter:input\n"
     ]
    }
   ],
   "source": [
    "def instantiate_linear(in_features, out_features, bias):\n",
    "    if bias is not None:\n",
    "        bias = True\n",
    "    return nn.Linear(\n",
    "        in_features=in_features,\n",
    "        out_features=out_features,\n",
    "        bias=bias)\n",
    "\n",
    "def redefine_linear_transform_pass(graph, pass_args=None):\n",
    "    main_config = pass_args.pop('config')\n",
    "    default = main_config.pop('default', None)\n",
    "    if default is None:\n",
    "       raise ValueError(f\"default value must be provided.\")\n",
    "    i = 0\n",
    "    for node in graph.fx_graph.nodes:\n",
    "        i += 1\n",
    "        # if node name is not matched, it won't be tracked\n",
    "        config = main_config.get(node.name, default)['config']\n",
    "        name = config.get(\"name\", None)\n",
    "        if name is not None:\n",
    "            ori_module = graph.modules[node.target]\n",
    "            in_features = ori_module.in_features\n",
    "            out_features = ori_module.out_features\n",
    "            bias = ori_module.bias\n",
    "            if name == \"output_only\":\n",
    "                out_features = out_features * config[\"channel_multiplier\"]\n",
    "            elif name == \"both\":\n",
    "                in_features = in_features * config[\"channel_multiplier\"]\n",
    "                out_features = out_features * config[\"channel_multiplier\"]\n",
    "            elif name == \"input_only\":\n",
    "                in_features = in_features * config[\"channel_multiplier\"]\n",
    "            elif name ==\"input_andoutput_only\":\n",
    "                in_features = in_features * config[\"channel_multiplier1\"]\n",
    "                out_features = out_features * config[\"channel_multiplier2\"]\n",
    "                print(\"inandout\")\n",
    "            \n",
    "            new_module = instantiate_linear(in_features, out_features, bias)\n",
    "                \n",
    "            parent_name, name = get_parent_name(node.target)\n",
    "            setattr(graph.modules[parent_name], name, new_module)\n",
    "    return graph, {}\n",
    "\n",
    "\n",
    "\n",
    "pass_config = {\n",
    "\"by\": \"name\",\n",
    "\"default\": {\"config\": {\"name\": None}},\n",
    "\"seq_blocks_2\": {\n",
    "    \"config\": {\n",
    "        \"name\": \"output_only\",\n",
    "        # weight\n",
    "        \"channel_multiplier\": 2,\n",
    "        }\n",
    "    },\n",
    "\"seq_blocks_3\": {\n",
    "    \"config\": {\n",
    "        \"name\": \"both\",\n",
    "        \"channel_multiplier\": 2,\n",
    "        }\n",
    "    },\n",
    "\"seq_blocks_4\": {\n",
    "    \"config\": {\n",
    "        \"name\": \"input_only\",\n",
    "        \"channel_multiplier\": 2,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# this performs the architecture transformation based on the config\n",
    "mg, _ = redefine_linear_transform_pass(\n",
    "    graph=mg, pass_args={\"config\": pass_config})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy and paste the above coding snippet and run your code. The modified network features linear layers expanded to double their size, yet it's unusual to sequence three linear layers consecutively without interposing any non-linear activations (do you know why?).\n",
    "\n",
    "So we are interested in a modified network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a new model\n",
    "class JSC_Three_Linear_Layers(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JSC_Three_Linear_Layers, self).__init__()\n",
    "        self.seq_blocks = nn.Sequential(\n",
    "            nn.BatchNorm1d(16),  # 0\n",
    "            nn.ReLU(16),  # 1\n",
    "            nn.Linear(16, 16),  # linear seq_2\n",
    "            nn.ReLU(16),  # 3\n",
    "            nn.Linear(16, 16),  # linear seq_4\n",
    "            nn.ReLU(16),  # 5\n",
    "            nn.Linear(16, 5),  # linear seq_6\n",
    "            nn.ReLU(5),  # 7\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq_blocks(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Can you edit your code, so that we can modify the above network to have layers expanded to double their sizes? Note: you will have to change the `ReLU` also.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:output\n",
      "counter:both\n",
      "counter:input\n"
     ]
    }
   ],
   "source": [
    "###Part 1\n",
    "import copy\n",
    "model = JSC_Three_Linear_Layers()\n",
    "\n",
    "# generate the mase graph and initialize node metadata\n",
    "mg = MaseGraph(model=model)\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "\n",
    "\n",
    "pass_config = {\n",
    "\"by\": \"name\",\n",
    "\"default\": {\"config\": {\"name\": None}},\n",
    "\"seq_blocks_2\": {\n",
    "    \"config\": {\n",
    "        \"name\": \"output_only\",\n",
    "        # weight\n",
    "        \"channel_multiplier\": 2,\n",
    "        }\n",
    "    },\n",
    "\n",
    "\"seq_blocks_4\": {\n",
    "    \"config\": {\n",
    "        \"name\": \"both\",\n",
    "        \"channel_multiplier\": 2,\n",
    "        }\n",
    "    },\n",
    "\n",
    "\"seq_blocks_6\": {\n",
    "    \"config\": {\n",
    "        \"name\": \"input_only\",\n",
    "        \"channel_multiplier\": 2,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "#this performs the architecture transformation based on the config\n",
    "mg, _ = redefine_linear_transform_pass(\n",
    "    graph=mg, pass_args={\"config\": copy.deepcopy(pass_config)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In `lab3`, we have implemented a grid search, can we use the grid search to search for the best channel multiplier value?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Part 2\n",
    "\n",
    "\n",
    "\n",
    "import copy\n",
    "# build a search space\n",
    "multipliers = [1, 2, 3, 4, 5, 6, 7]\n",
    "#multipliers = [2, 2, 2, 2]\n",
    "search_spaces = []\n",
    "for d in multipliers:\n",
    "    pass_config['seq_blocks_2']['config']['channel_multiplier'] = d\n",
    "    pass_config['seq_blocks_4']['config']['channel_multiplier'] = d\n",
    "    pass_config['seq_blocks_6']['config']['channel_multiplier'] = d\n",
    "    # dict.copy() and dict(dict) only perform shallow copies\n",
    "    # in fact, only primitive data types in python are doing implicit copy when a = b happens\n",
    "    search_spaces.append(copy.deepcopy(pass_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:output\n",
      "counter:both\n",
      "counter:input\n",
      "counter:output\n",
      "counter:both\n",
      "counter:input\n",
      "counter:output\n",
      "counter:both\n",
      "counter:input\n",
      "counter:output\n",
      "counter:both\n",
      "counter:input\n",
      "counter:output\n",
      "counter:both\n",
      "counter:input\n",
      "counter:output\n",
      "counter:both\n",
      "counter:input\n",
      "counter:output\n",
      "counter:both\n",
      "counter:input\n",
      "counter:output\n",
      "counter:both\n",
      "counter:input\n",
      "counter:output\n",
      "counter:both\n",
      "counter:input\n",
      "counter:output\n",
      "counter:both\n",
      "counter:input\n",
      "counter:output\n",
      "counter:both\n",
      "counter:input\n",
      "counter:output\n",
      "counter:both\n",
      "counter:input\n",
      "counter:output\n",
      "counter:both\n",
      "counter:input\n",
      "counter:output\n",
      "counter:both\n",
      "counter:input\n",
      "[tensor(0.1810), tensor(0.1589), tensor(0.2298), tensor(0.1833), tensor(0.2238), tensor(0.1810), tensor(0.2893)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "\n",
    "metric = MulticlassAccuracy(num_classes=5)\n",
    "num_batchs = 5\n",
    "# This first loop is basically our search strategy,\n",
    "# in this case, it is a simple brute force search\n",
    "\n",
    "recorded_accs = []\n",
    "\n",
    "for d in enumerate(search_spaces):\n",
    "    model = get_model(\n",
    "    model_name,\n",
    "    task=\"cls\",\n",
    "    dataset_info=data_module.dataset_info,\n",
    "    pretrained=False,\n",
    "    checkpoint = None)\n",
    "\n",
    "    #_ = model(**dummy_in)\n",
    "    # generate the mase graph and initialize node metadata\n",
    "    mg = MaseGraph(model=model)\n",
    "    mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "    mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n",
    "    mg, _ = add_software_metadata_analysis_pass(mg, None)\n",
    "    mg, _ = redefine_linear_transform_pass(graph=mg, pass_args=copy.deepcopy({\"config\": pass_config}))\n",
    "    j = 0\n",
    "\n",
    "     # this is the inner loop, where we also call it as a runner.\n",
    "    acc_avg, loss_avg = 0, 0\n",
    "    accs, losses = [], []\n",
    "    for inputs in data_module.train_dataloader():\n",
    "        xs, ys = inputs\n",
    "        preds = mg.model(xs)\n",
    "        loss = torch.nn.functional.cross_entropy(preds, ys)\n",
    "        acc = metric(preds, ys)\n",
    "        accs.append(acc)\n",
    "        losses.append(loss)\n",
    "        if j > num_batchs:\n",
    "            break\n",
    "        j += 1\n",
    "    acc_avg = sum(accs) / len(accs)\n",
    "    loss_avg = sum(losses) / len(losses)\n",
    "    recorded_accs.append(acc_avg)\n",
    "    mga, _ = redefine_linear_transform_pass(graph=mg, pass_args=copy.deepcopy({\"config\": pass_config}))\n",
    "print(recorded_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You may have noticed, one problem with the channel multiplier is that it scales all layers uniformly, ideally, we would like to be able to construct networks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new model\n",
    "class JSC_Three_Linear_Layers(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JSC_Three_Linear_Layers, self).__init__()\n",
    "        self.seq_blocks = nn.Sequential(\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(16),\n",
    "            nn.Linear(16, 32),  # output scaled by 2\n",
    "            nn.ReLU(32),  # scaled by 2\n",
    "            nn.Linear(32, 64),  # input scaled by 2 but output scaled by 4\n",
    "            nn.ReLU(64),  # scaled by 4\n",
    "            nn.Linear(64, 5),  # scaled by 4\n",
    "            nn.ReLU(5),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq_blocks(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:output\n",
      "counter:output\n",
      "counter:input\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pass_config = {\n",
    "\"by\": \"name\",\n",
    "\"default\": {\"config\": {\"name\": None}},\n",
    "\"seq_blocks_2\": {\n",
    "    \"config\": {\n",
    "        \"name\": \"output_only\",\n",
    "        # weight\n",
    "        \"channel_multiplier\": 2,\n",
    "        }\n",
    "    },\n",
    "\n",
    "\"seq_blocks_4\": {\n",
    "    \"config\": {\n",
    "        \"name\": \"input_andoutput_only\",\n",
    "        \"channel_multiplier1\": 2,\n",
    "        \"channel_multiplier2\": 2,\n",
    "        }\n",
    "    },\n",
    "\n",
    "\"seq_blocks_6\": {\n",
    "    \"config\": {\n",
    "        \"name\": \"input_only\",\n",
    "        \"channel_multiplier\": 4,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "model = JSC_Three_Linear_Layers()\n",
    "\n",
    "# generate the mase graph and initialize node metadata\n",
    "mg = MaseGraph(model=model)\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "mg, _ = redefine_linear_transform_pass(\n",
    "    graph=mg, pass_args={\"config\": copy.deepcopy(pass_config)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Integrate the search to the `chop` flow, so we can run it from the command line.\n",
    "\n",
    "## Optional Task (scaling the search to real networks)\n",
    "\n",
    "We have looked at how to search, on the architecture level, for a simple linear layer based network. MASE has the following components that you can have a look:\n",
    "\n",
    "- Cifar10 dataset\n",
    "- VGG, this is a variant used for CIFAR\n",
    "- TPE-based Search, implemented using Optuna\n",
    "\n",
    "Can you define a search space (maybe channel dimension) for the VGG network, and use the TPE-search to tune it?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
